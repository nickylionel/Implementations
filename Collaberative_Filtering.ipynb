{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collaborative Filtering \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook has implementation of Collaborative filtering by optimizing a single cost function to learn both User features and Product features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.sparse import csr_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing User featuers and Product features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "n_u is the number of unique users.\n",
    "\n",
    "n_m is the number of unique products.\n",
    "\n",
    "n_f is the size of the feature vector that we want to use for both the user and the product.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use normal distribution for both the user and product features with mean 0 and standard deviation 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_features(n_u, n_m, n_f):\n",
    "    # mean and standard deviation\n",
    "    mu, sigma = 0, 0.1 \n",
    "    user_features = np.random.normal(mu, sigma, (n_u, n_f))\n",
    "    prod_features = np.random.normal(mu, sigma, (n_m, n_f))\n",
    "    return user_features, prod_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting Ratings for given users and products"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since, taking a dot product of user_features and productfeatures will result in a $n&lt;/em&gt;{u} x n_{m}$ matrix which runs out of memory. We use scipy sparse matrix implementation library to avoid memory error.\n",
    "\n",
    "The inputs to predict_ratings method are $user\\_features, product\\_features$ and $users, products$. The $users, products$ are list of users who rated respective products. This can be used to index user_features and product_features to ensure that the result matrix is sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_ratings(user_features, prod_features, users, products):\n",
    "    predicted_ratings = np.sum(np.multiply(user_features[users,:], prod_features[products,:]), axis=1)\n",
    "    return csr_matrix((predicted_ratings, (users, products)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the features and predictions of ratings based on the features.\n",
    "\n",
    "Cost function J is given by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(user_features, prod_features, prediction_sparse_matrix, ratings_sparse_matrix, lambd=0.1):\n",
    "    user_regularization = (lambd/2)*np.sum(np.sum(np.square(user_features)))\n",
    "    song_regularization = (lambd/2)*np.sum(np.sum(np.square(song_features)))\n",
    "    cost = (1/2)*(prediction_sparse_matrix - ratings_sparse_matrix).power(2).sum() + user_regularization + song_regularization\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To learn the parameters, we have to minimize the cost function given above given above with respect to both $ x^{(i)} $ and $ c^{(i)} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradients(user_features, song_features, predicted_ratings_sparse, actual_ratings_sparse, lambd=0.1):\n",
    "    prediction_offset_sparse = predicted_ratings_sparse - actual_ratings_sparse\n",
    "    d_users = csr_matrix.dot(prediction_offset_sparse, song_features) + lambd*user_features\n",
    "    d_songs = csr_matrix.dot(csr_matrix.transpose(prediction_offset_sparse), user_features) + lambd*song_features\n",
    "    return d_users, d_songs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Updating parameters by using computed gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_features(user_features, song_features, d_users, d_songs, learning_rate=0.01):\n",
    "    user_features = user_features - learning_rate*d_users\n",
    "    song_features = song_features - learning_rate*d_songs\n",
    "    return user_features, song_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comupte error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is similar to the first part of the cost function. It is the Square root of sum of squared errors for each prediction. This is a useful evaluation metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_error(true_ratings, predicted_ratings):\n",
    "    return np.sqrt((predicted_ratings - true_ratings).power(2).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this algorithm, we will use the songs dataset from kaggle.\n",
    "To split the train and test data, we randomly select one observation for each user as a test observation and use the square root of sum of squared errors to determine the goodness of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading data\n",
    "data = pd.read_csv('data/songsDataset.csv')\n",
    "data['songIDx'] = data['songID'].astype('category')\n",
    "data['songIDx'] = data['songIDx'].cat.codes\n",
    "data['pkey'] = np.arange(data.shape[0])\n",
    "\n",
    "# Selecting one rating by each user to test properly\n",
    "test_data = data.groupby(['userID'], as_index=False).apply(lambda x : x.loc[np.random.choice(x.index, replace=False)])\n",
    "train_data = data.loc[~data['pkey'].isin(test_data['pkey']), :]   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Declaring some constants and verifying the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we will use,\n",
    "$n\\_f$ to denote number of features.\n",
    "$n\\_u$ to denote number of users.\n",
    "$n\\_m$ to denote number of songs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_f = 5\n",
    "n_u = data['userID'].unique().shape[0]\n",
    "n_m = data['songID'].unique().shape[0]\n",
    "num_epochs = 51\n",
    "lambd = 1\n",
    "learning_rate = 0.01\n",
    "\n",
    "print('Total number of observations in training data is {}'.format(train_data.shape[0]))\n",
    "print('Total number of observations in testing data is {}'.format(test_data.shape[0]))\n",
    "print('Number of features for both products and users is {}'.format(n_f))\n",
    "print('Number of unique users in the data is {}'.format(n_u))\n",
    "print('Number of unique products in the data is {}'.format(n_m))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the ratings sparse matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use scipy sparse matrix library to construct actual ratings sparse matrix for both the training and the testing dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constructing the train_ratings sparse matrix\n",
    "train_users = train_data['userID']\n",
    "train_songs = train_data['songIDx']\n",
    "train_ratings = csr_matrix((train_data['rating'], (train_users, train_songs)))\n",
    "\n",
    "# Constructing the test_ratings sparse matrix\n",
    "test_users = test_data['userID']\n",
    "test_songs = test_data['songIDx']\n",
    "test_ratings = csr_matrix((test_data['rating'], (test_users, test_songs)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "user_features, song_features = initialize_features(n_u, n_m, n_f)\n",
    "for eph in range(num_epochs):\n",
    "    predicted_ratings_sparse = predict_ratings(user_features, song_features, train_users, train_songs)\n",
    "    print(predicted_ratings_sparse[0,6706])\n",
    "    if(eph % 2 == 0):\n",
    "        print(\"Cost at epoch {} is {}\".format(eph, compute_cost(user_features, song_features, predicted_ratings_sparse, train_ratings, lambd)))\n",
    "    if(eph % 5 == 0):\n",
    "        test_predicts = predict_ratings(user_features, song_features, test_users, test_songs)\n",
    "        print(\"Mean squared error on test data after epoch {} is {}\".format(eph, compute_error(test_ratings, test_predicts)))\n",
    "        print(\"Mean squared error on train data after epoch {} is {}\".format(eph, compute_error(train_ratings, predicted_ratings_sparse)))\n",
    "    d_users, d_songs = compute_gradients(user_features, song_features, predicted_ratings_sparse, train_ratings, lambd)\n",
    "    user_features, song_features = update_features(user_features, song_features, d_users, d_songs, learning_rate)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
